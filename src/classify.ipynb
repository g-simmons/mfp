{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from xgboost import XGBClassifier\n",
    "import json\n",
    "from svmclassifier import SVMClassifier\n",
    "from nnclassifier import NNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the one-hot encoded data\n",
    "Use one-hot token method, since this performed better than category using the taxonomy  \n",
    "`onehot_feature.json` is created using `onehot.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/onehot_feature.json\", 'r') as f:\n",
    "    users = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove \"on target\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [user for user in users if user[\"label\"] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [user[\"label\"] for user in data]\n",
    "X = [np.array(user[\"feature\"]) for user in data]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with one-hot encoding input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = {'accuracy':'accuracy',\n",
    "           'precision':make_scorer(precision_score,average='micro'),\n",
    "           'recall':make_scorer(recall_score,average='micro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc_scores_one_hot = cross_validate(svc,X,y,scoring=scoring,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_accuracy\n",
      "Average test_accuracy\n",
      "0.6608165790049986\n",
      "Std of test_accuracy\n",
      "0.007556640457673412\n",
      "\n",
      "test_precision\n",
      "Average test_precision\n",
      "0.6608165790049986\n",
      "Std of test_precision\n",
      "0.007556640457673412\n",
      "\n",
      "test_recall\n",
      "Average test_recall\n",
      "0.6608165790049986\n",
      "Std of test_recall\n",
      "0.007556640457673412\n"
     ]
    }
   ],
   "source": [
    "for metric in ['test_accuracy','test_precision','test_recall']:\n",
    "    print('\\n' + metric)\n",
    "    print('Average ' + metric)\n",
    "    print(round(np.mean(svc_scores_one_hot[metric]),2))\n",
    "    print('Std of ' + metric)\n",
    "    print(round(np.std(svc_scores_one_hot[metric]),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with one-hot encoding input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
    "                             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "                             hidden_layer_sizes=(200), learning_rate='constant',\n",
    "                             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "                             nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "                             solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "                             warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_scores_one_hot = cross_validate(mlp,X,y,scoring=scoring,cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare vectorized data for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model input\n",
    "This input is constructed using `construct_classifier_input_data.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/food_vectors_w2v.csv',header=None)\n",
    "data = data.rename(columns={0:'user',1:'date',102:'goal',103:'actual',104:'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and aggregation by user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the original paper, we discard days where less than 100 calories were logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[data['actual'] > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove \"on target\" class\n",
    "Following the original paper, we discard the 'on_target' label, simplifying the problem to binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[data['label'] != 'on_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the data by user\n",
    "We group the data by user, aggregate the vectors using the mean, and aggregate the users by taking the modal class.  \n",
    "I.e. if most days that are recorded for a given user have been labeled \"below\", we label this user as \"below\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = data.groupby('user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the number of users with more than 30 days logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(groups['user'].count()[lambda x: x >= 30].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a new dataframe of the aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_y = groups['label'].agg(lambda x:x.value_counts().index[0])\n",
    "grouped_x = groups.mean()\n",
    "grouped = grouped_x.copy()\n",
    "grouped['label'] = grouped_y\n",
    "grouped_above_30 = grouped.loc[groups['user'].count()[lambda x: x >= 30].index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of users in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Above: 1303\n",
      "Below: 3478\n"
     ]
    }
   ],
   "source": [
    "print(\"Above: \" + str(sum(grouped_above_30['label'] == 'above')))\n",
    "print(\"Below: \" + str(sum(grouped_above_30['label'] == 'below')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the classes\n",
    "We should try classification with the classes balanced, as the original paper did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# balance the classes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into model input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = grouped_above_30.loc[:,'label']\n",
    "X = grouped_above_30.iloc[:,0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM using vector input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = {'accuracy':'accuracy',\n",
    "           'precision':make_scorer(precision_score,average='micro'),\n",
    "           'recall':make_scorer(recall_score,average='micro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc_scores = cross_validate(svc,X,y,scoring=scoring,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_accuracy\n",
      "Average test_accuracy\n",
      "0.66\n",
      "Std of test_accuracy\n",
      "0.011\n",
      "\n",
      "test_precision\n",
      "Average test_precision\n",
      "0.66\n",
      "Std of test_precision\n",
      "0.011\n",
      "\n",
      "test_recall\n",
      "Average test_recall\n",
      "0.66\n",
      "Std of test_recall\n",
      "0.011\n"
     ]
    }
   ],
   "source": [
    "for metric in ['test_accuracy','test_precision','test_recall']:\n",
    "    print('\\n' + metric)\n",
    "    print('Average ' + metric)\n",
    "    print(round(np.mean(svc_scores[metric]),2))\n",
    "    print('Std of ' + metric)\n",
    "    print(round(np.std(svc_scores[metric]),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN using vector input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   16.4s finished\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
    "                             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "                             hidden_layer_sizes=(200), learning_rate='constant',\n",
    "                             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "                             nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "                             solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "                             warm_start=False)\n",
    "mlp_scores = cross_validate(mlp,X,y,scoring=scoring,cv=3,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_accuracy\n",
      "Average test_accuracy\n",
      "0.7134519426356559\n",
      "Std of test_accuracy\n",
      "0.004883994427665733\n",
      "\n",
      "test_precision\n",
      "Average test_precision\n",
      "0.6333429769555584\n",
      "Std of test_precision\n",
      "0.0038402151501620244\n",
      "\n",
      "test_recall\n",
      "Average test_recall\n",
      "0.6261872793181188\n",
      "Std of test_recall\n",
      "0.0034801098281369482\n"
     ]
    }
   ],
   "source": [
    "for metric in ['test_accuracy','test_precision','test_recall']:\n",
    "    print('\\n' + metric)\n",
    "    print('Average ' + metric)\n",
    "    print(np.mean(mlp_scores[metric]))\n",
    "    print('Std of ' + metric)\n",
    "    print(np.std(mlp_scores[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models and performance results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(svc,open('../predictive_models/svm_model.pkl','wb'))\n",
    "pkl.dump(scores,open('../predictive_models/svm_5_fold_scores.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importances using vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute 'dual_coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-edf98b05db2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf_importances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mcoef_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    485\u001b[0m                                  'linear kernel')\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m         \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_coef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;31m# coef_ being a read-only property, it's better to mark the value as\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_get_coef\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_coef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m             \u001b[1;31m# binary classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SVC' object has no attribute 'dual_coef_'"
     ]
    }
   ],
   "source": [
    "f_importances(svc.coef_, X.columns[0:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
