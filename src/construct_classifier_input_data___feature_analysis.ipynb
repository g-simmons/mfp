{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import FastText\n",
    "from epochlogger import EpochLogger\n",
    "from sentencegenerator import SentenceGenerator\n",
    "import nltk\n",
    "import re\n",
    "from numpy import float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/mfp-diaries.tsv',sep='\\t',header=None,chunksize=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the W2V and FastText models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load('../word2vecmodels/foods_w2v_window6_mc10.model')\n",
    "fasttext = FastText.load('../fasttextmodels/foods_fasttext_window6_mc10.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a unique row ID based on user and date\n",
    "We originally planned to do this, but realized we later need to aggregate by user anyway. This data is separated back out when we construct the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_id(row):\n",
    "    date = row[1][1]\n",
    "    user = row[1][0]\n",
    "    ID = str(user) + '-' + str(date)\n",
    "    return [user, date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidaton functions used to group vectors by sum or mean\n",
    "If the item to be grouped is empty, we return an empty vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def consolidate_sum(list_of_vectors):\n",
    "    if len(list_of_vectors) == 0:\n",
    "        return [np.zeros(100)]\n",
    "    if len(list_of_vectors) > 1:\n",
    "        return np.sum(list_of_vectors,axis=0)\n",
    "    else:\n",
    "        return np.asarray(list_of_vectors[0])\n",
    "    \n",
    "def consolidate_mean(list_of_vectors):\n",
    "    if len(list_of_vectors) == 0:\n",
    "        return [np.zeros(100)]\n",
    "    if len(list_of_vectors) > 1:\n",
    "        return np.mean(list_of_vectors,axis=0)\n",
    "    else:\n",
    "        return np.asarray(list_of_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diary_vector(row,model):\n",
    "    meal_vectors = []\n",
    "    diary = json.loads(row[1][2])\n",
    "    for meal in diary:\n",
    "        meal_vectors.append(get_meal_vector(meal,model))\n",
    "    diary_vector = consolidate_sum(meal_vectors)\n",
    "    return diary_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the vector representing a meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_meal_vector(meal,model):\n",
    "    dish_vectors = []\n",
    "    for dish in meal['dishes']:\n",
    "        dish_vectors.append(get_dish_vector(dish,model))\n",
    "    if len(dish_vectors) == 0:\n",
    "        meal_vector =  np.zeros(100).reshape(100,)\n",
    "    else:\n",
    "        meal_vector = consolidate_sum(dish_vectors)\n",
    "    assert meal_vector.shape == (100,)\n",
    "    return meal_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the vector representing one dish (food item) within a meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dish_vector(dish,model):\n",
    "    # Handle 'Quick Added Calories\n",
    "    if dish['name'][0:20] == 'Quick Added Calories':\n",
    "        name = 'Quick Added Calories'\n",
    "    else:\n",
    "        name, quant = dish['name'].split(',',maxsplit=1)\n",
    "    # split based on delimiting characters\n",
    "    tokens = re.split(\"[, \\-!?*+()012345678~9&%=/\\\"#.>^<:]+\",name) \n",
    "    #change to lowercase\n",
    "    tokens_lower =  [token.lower() for token in tokens if len(token) > 2]\n",
    "    \n",
    "    # get dish vector\n",
    "    word_vectors = []\n",
    "    for word in tokens_lower:\n",
    "        if word in model.wv.vocab:\n",
    "            word_vectors.append(model.wv[word])\n",
    "    if len(word_vectors) == 0:\n",
    "        dish_vector = np.zeros(100).reshape(100,)\n",
    "    else:\n",
    "        dish_vector = consolidate_mean(word_vectors)\n",
    "    assert dish_vector.shape == (100,)\n",
    "    return dish_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label the user based on their goal vs actual calories\n",
    "We follow the labeling convention in the original paper, labeling users who are below their goal by a >=20% margin to be \"below\", under their goal within the 20% margin as \"on_target\", and users who are above their goal as \"above\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_label(row):\n",
    "    summary = json.loads(row[1][3])\n",
    "    if len(summary['goal']) > 0 and len(summary['total']) > 0:\n",
    "        goal_cal = summary['goal'][0]['value']\n",
    "        total_cal = summary['total'][0]['value']\n",
    "        if goal_cal == 0:\n",
    "            return None\n",
    "        if total_cal > goal_cal:\n",
    "            success = 'above'\n",
    "        else:\n",
    "            if (goal_cal-total_cal)/goal_cal > 0.2:\n",
    "                success = 'below'\n",
    "            else:\n",
    "                success = 'on_target'\n",
    "    else:\n",
    "        return None\n",
    "    return [goal_cal,total_cal,success]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the original data to contruct the classifier input using the Word2Vec vectors\n",
    "\n",
    "- Output rows are in the format `[user][date][.......fasttext vector........][goal][actual][label]`\n",
    "\n",
    "- \"mixed_1\" in the file name indicates the mix of aggregation functions. We found that averaging words to get foods, summing foods to get meals, and summing meals to get days works best. We called this mix 1, as opposed to other mixes of sum and mean aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through the original data to contruct the classifier input using the FastText vectors\n",
    "\n",
    "- Output rows are in the format `[user][date][.......fasttext vector........][goal][actual][label]`\n",
    "\n",
    "- \"mixed_1\" in the file name indicates the mix of aggregation functions. We found that averaging words to get foods, summing foods to get meals, and summing meals to get days works best. We called this mix 1, as opposed to other mixes of sum and mean aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587187\r"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open(\"../data/food_vectors_fasttext.csv\", \"w\") as sentences:\n",
    "    for chunk in data:\n",
    "        for row in chunk.iterrows():            \n",
    "            ID = create_id(row)\n",
    "            diary_vector = get_diary_vector(row,fasttext)\n",
    "            label = create_label(row)\n",
    "            if label:\n",
    "                final_vec = [str(ID[0]),str(ID[1])]\n",
    "                try:\n",
    "                    len(diary_vector[0])\n",
    "                    diary_vector = diary_vector[0]\n",
    "                except:\n",
    "                    pass\n",
    "                for elem in diary_vector:\n",
    "                    final_vec.append(str(elem))\n",
    "                for elem in label:\n",
    "                    final_vec.append(str(elem))\n",
    "                sentences.write(','.join(final_vec)+'\\n')\n",
    "            i += 1\n",
    "            print(i,end='\\r')\n",
    "            \n",
    "sentences.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "with open(\"../data/food_vectors_fasttext.csv\", \"w\") as sentences:\n",
    "    for chunk in data:\n",
    "        for row in chunk.iterrows():            \n",
    "            ID = create_id(row)\n",
    "            diary_vector = get_diary_vector(row,fasttext)\n",
    "            label = create_label(row)\n",
    "            if label:\n",
    "                final_vec = [str(ID[0]),str(ID[1])]\n",
    "                try:\n",
    "                    len(diary_vector[0])\n",
    "                    diary_vector = diary_vector[0]\n",
    "                except:\n",
    "                    pass\n",
    "                for elem in diary_vector:\n",
    "                    final_vec.append(str(elem))\n",
    "                for elem in label:\n",
    "                    final_vec.append(str(elem))\n",
    "                sentences.write(','.join(final_vec)+'\\n')\n",
    "            i += 1\n",
    "            print(i,end='\\r')\n",
    "            \n",
    "sentences.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get vector for each individual food name for feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587187\r"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open(\"../data/food_vectors_individual_fasttext.csv\", \"w\",encoding='utf-8') as sentences:\n",
    "    for chunk in data:\n",
    "        for row in chunk.iterrows():            \n",
    "            ID = create_id(row)\n",
    "            diary = json.loads(row[1][2])\n",
    "            for meal in diary:\n",
    "                for dish in meal['dishes']:\n",
    "                    final_vec = [str(ID[0]),str(ID[1])]\n",
    "                    if dish['name'][0:20] == 'Quick Added Calories':\n",
    "                        name = 'Quick Added Calories'\n",
    "                    else:\n",
    "                        name, quant = dish['name'].split(',',maxsplit=1)\n",
    "                    # split based on delimiting characters\n",
    "                    tokens = re.split(\"[, \\-!?*+()012345678~9&%=/\\\"#.>^<:]+\",name) \n",
    "                    #change to lowercase\n",
    "                    tokens_lower =  [token.lower() for token in tokens if len(token) > 2]\n",
    "                    final_vec.append(' '.join(tokens_lower))\n",
    "                    dish_vector = get_dish_vector(dish,fasttext)\n",
    "                    for elem in dish_vector:\n",
    "                        final_vec.append(str(elem))\n",
    "                    sentences.write(','.join(final_vec)+'\\n')\n",
    "            i += 1\n",
    "            print(i,end='\\r')\n",
    "            \n",
    "sentences.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
